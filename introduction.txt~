\\Wireless sensor networks have been of increasingly great interest due to the wide variety of applications in which this technology can be applied. Practicality can range from long distance navigational systems and large scale military surveillance to localized contact points for emergency services and even small scale biosensors for monitoring individual human performance.
\\While the demand for and scope of wireless sensor networks continue to grow, the need for identifying a node's location within such a network becomes one of great importance. However, achieving localization with minimal power, minimal overall cost, and high accuracy remains one of the greatest challenges in the area of wireless sensor networks. While many novel algorithms have been developed in recent years to address this balance, most of these are extensively iterative, follow an exhaustive guess-and-check approach, or both. This is primarily due to the incomplete or unrelatable nature of the data that is initially processed. 
\\Types of data that are particularly useful in localization include received signal strength (RSS), angle of arrival (AOA), time of arrival (TOA), and time difference of arrival (TDOA). [elaborate on these concepts] Since most of these data types do not appear to bear an explicit relation with one another, the concept of data fusion becomes one of particular importance. 

\section{Overview of Data Fusion}

\\Data fusion is a valuable and streamlined process that involves obtaining many different types of data from a single common application and integrating them into a unified, consistent representation. While an array of similar data is relatively simple to implement in the context of data fusion, more relevant aspects of data fusion are the result of widely varying types of data that derive from a multitude of different sensor types. In the event of such a large magnitude of complexity within the field of data fusion, a significant amount of creativity and elaboration is needed to solve such a problem. To the benefit of this research, one problem solving technique exists to provide a powerful and straightforward approach to fusing dissimilar data types, although its application of the field of wireless sensor networks, as well as that of sensors in general, is one of recent development, whose potential therefore remains largely untapped. As the title of this thesis suggests, this technique is none other than Dempster-Shafer Evidence Theory.

\section{Overview of Dempster-Shafer Theory}

\\Dempster-Shafer Theory, herein referred as DS Theory or DST, is a statistical framework based on Baynesian probability. While Baynesian statistics rely primarily upon combinations of internal probability factors within a system, usually through the use of propositions or random variables, DS Theory is contingent upon external evidence factors, each one typically consisting of a range of data in which the desired output value can be found and a probability of confidence that the data range is in fact reliable. Because DST is still a relatively new concept in the scope of this research, a brief example that shows a fundamental contrast between DST-based probability and its traditional Baynesian counterpart is provided for the benefit of the reader.

\section{DS Theory Example}

\\To simplify both methods in the context of a practical example, suppose there exists a car that is due to fail at some given moment and has three factors that are the most likely to result in the vehicle’s failure, which will only occur if all three of these components fail individually. In the context of this example, the three components are the car’s worn out spark plugs, depleting oil, and aging transmission fluid, herein referred as Components 1, 2, and 3, respectively. The objective of this problem is now to find out the overall probability of the car failing within one week. The Baynesian approach to solving this problem would involve measuring the failure rates of each individual component and combining the factors to determine the overall system failure probability. For the sake of having specific values, suppose the probabilities of Components 1, 2, and 3 respectively failing within one week are .6, .4, and .2. By use of combinational logic in the form of OR gates (as each component’s failure rate is independent of one another), we can conclude that the combined probability of the car failing within one week to be .6 * .4 * .2 = .048, or 4.8%. We have now obtained the probability of system failure for a fixed length of time, but if we were to go beyond this context and find more meaningful information, such as the length of time before reaching 100% failure, a different approach would be needed. 
\\To apply the same example with DS Theory, we will now obtain external evidence factors, which in the context of this problem will be: three mechanics, herein referred as Mechanics A, B, and C. Each mechanic inspects the vehicle as a whole, including all three aforementioned components and any other possible aspect of the car and then give an interval of time at which the vehicle is expected to fail as well as the degree of confidence in which the mechanic is certain of these time values. Suppose the vehicle is first taken to its original dealership, which has two mechanics, A and B, that specialize in the exact make and model of this vehicle and hence are highly confident in their predicted time range in which the car will fail, despite the fact that A’s time range differs from that of B. Mechanic A concludes that total failure will occur between 11 and 18 days in the future, while Mechanic B is certain that failure will happen between 14 and 21 days from the present. Next, the car is taken to a more general purpose, non-dealer specific repair shop, whose expert, Mechanic C, is less familiar with the specific make and model of this car when compared to the expertise of Mechanics A and B and thus concludes at half the confidence level of the other experts that vehicle-level failure will occur between 10 and 20 days into the future. 
\\Now that all confidence levels have been compared, we can deduce that A and B have equal confidence levels of .4 each, and that C has a confidence level of .2 (half that of the other mechanics). With all of this data intact, we have gathered the evidence factors in the form of [lower bound, upper bound, confidence] to be [11, 18, .4] for A, [14, 21, .4] for B, and [10, 20, .2] for C. By the conditions associated with DS Theory we can now conclude that the plausibility of vehicular failure to begin after 14 days and the belief of total failure to begin after 21 days. In other words, while we were able to use Baynesian probability to determine the probability of a car failing after the exact elapsed time of one week, DST allowed us to go a step further and find the amount of time at which one hundred percent failure was imminent, which in this case was three weeks.

\section{Proposed Localization Techniques}

\\Fault or failure prediction is just one of many possible applications that can be enhanced with DS Theory. Other relevant applications, ones that are more closely related with wireless sensor networks and with the aim and scope of this research, include the use of different node measurements (RSS, AOA, etc.) to predict the distance from a node to a monitoring station, [content from Chapter 4 algorithm goes here], and [content from Chapter 5 algorithm goes here.]. From these potential problem setups, three novel localization schemes are proposed in the body of this thesis.
\\The first technique is a binary decision mechanism with primary inputs consisting of a county location and various types of node measurements, such as RSS, AOA, and standby, and secondary inputs consisting of monitor data, probability weights, and feature-specific range criteria. This algorithm answers with a simple yes or no output the following question: given a county location and a set of measurements, do the given measurements belong to the given county? [Elaboration on first algorithm goes here.]
\\The second method is an algorithm for localization of a given county. Given primary inputs of a single set of various types of node measurements as described to the previous technique and a list of all county locations (without direct indication of which county belongs to the data) and also given secondary inputs of [secondary inputs go here], this algorithm looks for the county that most closely corresponds to said measurements, building upon the binary decision mechanism from before but taking the output a step further by giving a closely related location rather than a simple yes or no. [Elaboration of second algorithm goes here.]
\\Lastly, a technique is proposed that matches all counties in a network with all corresponding measurement sets. This method is similar in implementation to the second technique except that all measurement sets are given in no particular order so that the list of counties described previous has no direct correlation as to which which data set goes to which county.


















